% © 2025 Daniel P. Madden. Custodial Edition – AI OSI Stack v5.0-open-core.
% Unauthorized reproductions or derivatives are not recognized custodial works.
% Refer to CANONICAL_PROVENANCE.yaml for official verification.
% SPDX-License-Identifier: CC-BY-SA-4.0

% Implements: schemas/aeip/modelcard-schema.json
% AI OSI Stack v5 — Canonical Deepbuild
% Chapter 06 — Layer 3: Model Development
% Sources: Version 5 Draft, AEIP v1, Update Plans 4, 7, 9, Appendix F, Persona Architecture v2

\narrative{
Layer 3 governs how models are designed, trained, evaluated, and readied for deployment. It connects ethical charters and data
stewardship to engineering practice, ensuring that models reflect civic mandates and remain auditable throughout their lifecycle.
The narrative shows how Version 5 integrates AEIP workflows, fairness protocols, and persona-based evaluations to create
accountable models.
}

\subsection*{Triple Register}
\textbf{Narrative Intent:} Layer 3 addresses the opacity of model engineering by prescribing transparent development records that align safety, accountability, and iterative improvement.
\textbf{Normative Clauses:}
\begin{itemize}
\item Engineers \shall{} document architectural choices in \texttt{schemas/aeip/modelcard-schema.json} before models exit sandbox testing.
\item Product owners \shall{} supplement release packages with \texttt{schemas/modelcard-template.yaml} attachments so evaluators can reuse evidence structures.
\item Risk reviewers \should{} link mitigation decisions to \texttt{schemas/decision-rationale-record.jsonld} entries for traceability across layers.
\end{itemize}
\textbf{Plain-Speak Summary:} This layer makes model building legible to outsiders. It requires structured model cards and decision logs before release. Teams learn which forms to use so audits are repeatable. The process keeps technical improvements tied to governance expectations.


\section*{Design Specification}
\narrative{
Model development begins with detailed design specifications translating civic mandates and ethical charters into requirements.
Update Plan 4 introduced specification templates that tie objectives to measurable obligations. Designers collaborate with
community representatives to ensure that success criteria reflect societal goals rather than mere technical performance.
}
\normative{
Every model \shall{} have a documented design specification referencing relevant mandates, charters, and appendices. Specifications
MUST include intended use, prohibited uses, performance thresholds, fairness targets, interpretability requirements, and fallback
procedures. Documents \shall{} be logged in (AEIP §O.3) and \shall{} undergo community review before development begins.
}

\section*{Training Data Governance}
\narrative{
Layer 3 builds on Layer 2 by ensuring that only authorised, high-quality data enters the training pipeline. The narrative details
how data is curated, labelled, and partitioned with community oversight. Persona Architecture v2 contributes persona-aligned
validation sets to test behaviour under diverse perspectives.
}
\normative{
Training datasets \shall{} be sourced exclusively from inventories approved under Layer 2. Data preparation workflows MUST document
transformations, labelling procedures, and quality checks in (AEIP §O.4). Any synthetic data generation \shall{} disclose methods and
risk analyses. Community reviewers \shall{} have opportunities to audit dataset composition and MUST approve usage for high-risk
applications.
}

\section*{Model Training and Experimentation}
\narrative{
Training processes are conducted transparently. Experiment tracking systems record hyperparameters, evaluation metrics, and
resource usage. The narrative shows how AEIP receipts capture each experiment, enabling auditors to reproduce or review decisions.
Update Plan 7 emphasised controlled experimentation to prevent unapproved models from drifting into production.
}
\normative{
All training runs \shall{} be registered in (AEIP §O.4) with configuration files, dataset references, and responsible personnel.
Experimentation environments MUST enforce access controls and \shall{} prevent exporting models without authorization. When high-risk
changes occur, oversight councils \shall{} review experiment logs before approving progression to evaluation. Untracked experiments
\shall{} be considered non-compliant and MUST be quarantined.
}

\section*{Evaluation and Validation}
\narrative{
Evaluation ensures models meet civic expectations. Tests include accuracy, robustness, fairness, privacy impact, and persona-based
qualitative reviews. The narrative references Appendix F fairness metrics and community scenario testing. Validation results inform
whether a model is ready for governance publication and operational integration.
}
\normative{
Evaluation protocols \shall{} cover quantitative and qualitative metrics aligned with charter commitments. Results MUST be documented
in (AEIP §O.5) including methodology, datasets, statistical significance, and reviewer notes. Models \shall{not} advance if they fail
to meet fairness or safety thresholds. Validation teams \shall{} include community observers for sensitive applications. Exceptions
\may{} be granted under Appendix B emergency procedures but \shall{} undergo retrospective review.
}

\section*{Documentation and Explainability}
\narrative{
Explainability makes models intelligible to stakeholders. Layer 3 requires model cards, interpretability analyses, and narrative
explanations accessible to civic audiences. Persona Architecture v2 helps tailor explanations to community contexts. The narrative
highlights how transparent documentation builds trust and facilitates oversight.
}
\normative{
Each model \shall{} ship with documentation packages, including model cards, decision traces, and limitations. Explanations MUST be
available in technical and civic formats and \shall{} reference charter clauses. Documentation \shall{} be stored in public registers
unless restricted by privacy constraints; in such cases, summaries \shall{} be provided. Implementers \shall{} update documentation after
significant changes and MUST log revisions in (AEIP §O.6).
}

\section*{Release Management}
\narrative{
Before models enter operations, release management ensures readiness. The narrative describes readiness reviews involving civic
mandates, ethical charters, and data stewardship verifications. Governance Publication teams prepare announcements, and AEIP
registries capture version hashes and attestation signatures.
}
\normative{
Release decisions \shall{} require sign-off from custodians, oversight councils, and community reviewers. Approval packages MUST
include design specifications, training logs, evaluation reports, and risk mitigations. Release hashes \shall{} be published in
\texttt{INTEGRITY\_NOTICE.md} and recorded in (AEIP §O.5). Deployments \shall{not} proceed without completed release documentation and
public notification through Layer 7 channels.
}

\n\subsection*{Verification and Enforcement}
Conformance is evidenced through artefacts \texttt{schemas/aeip/modelcard-schema.json}, \texttt{schemas/modelcard-template.yaml}, and \texttt{schemas/decision-rationale-record.jsonld} and corresponding AEIP audit records.
\input{chapters/generated/layer3-control-table}
\clearpage