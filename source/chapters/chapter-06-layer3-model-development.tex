% Implements: schemas/aeip/modelcard-schema.json
% AI OSI Stack v5 — Canonical Deepbuild
% Chapter 06 — Layer 3: Model Development
% Sources: Version 5 Draft, AEIP v1, Update Plans 4, 7, 9, Appendix F, Persona Architecture v2

\narrative{
Layer 3 governs how models are designed, trained, evaluated, and readied for deployment. It connects ethical charters and data
stewardship to engineering practice, ensuring that models reflect civic mandates and remain auditable throughout their lifecycle.
The narrative shows how Version 5 integrates AEIP workflows, fairness protocols, and persona-based evaluations to create
accountable models.
}

\section*{Design Specification}
\narrative{
Model development begins with detailed design specifications translating civic mandates and ethical charters into requirements.
Update Plan 4 introduced specification templates that tie objectives to measurable obligations. Designers collaborate with
community representatives to ensure that success criteria reflect societal goals rather than mere technical performance.
}
\normative{
Every model SHALL have a documented design specification referencing relevant mandates, charters, and appendices. Specifications
MUST include intended use, prohibited uses, performance thresholds, fairness targets, interpretability requirements, and fallback
procedures. Documents SHALL be logged in (AEIP §O.3) and SHALL undergo community review before development begins.
}

\section*{Training Data Governance}
\narrative{
Layer 3 builds on Layer 2 by ensuring that only authorised, high-quality data enters the training pipeline. The narrative details
how data is curated, labelled, and partitioned with community oversight. Persona Architecture v2 contributes persona-aligned
validation sets to test behaviour under diverse perspectives.
}
\normative{
Training datasets SHALL be sourced exclusively from inventories approved under Layer 2. Data preparation workflows MUST document
transformations, labelling procedures, and quality checks in (AEIP §O.4). Any synthetic data generation SHALL disclose methods and
risk analyses. Community reviewers SHALL have opportunities to audit dataset composition and MUST approve usage for high-risk
applications.
}

\section*{Model Training and Experimentation}
\narrative{
Training processes are conducted transparently. Experiment tracking systems record hyperparameters, evaluation metrics, and
resource usage. The narrative shows how AEIP receipts capture each experiment, enabling auditors to reproduce or review decisions.
Update Plan 7 emphasised controlled experimentation to prevent unapproved models from drifting into production.
}
\normative{
All training runs SHALL be registered in (AEIP §O.4) with configuration files, dataset references, and responsible personnel.
Experimentation environments MUST enforce access controls and SHALL prevent exporting models without authorization. When high-risk
changes occur, oversight councils SHALL review experiment logs before approving progression to evaluation. Untracked experiments
SHALL be considered non-compliant and MUST be quarantined.
}

\section*{Evaluation and Validation}
\narrative{
Evaluation ensures models meet civic expectations. Tests include accuracy, robustness, fairness, privacy impact, and persona-based
qualitative reviews. The narrative references Appendix F fairness metrics and community scenario testing. Validation results inform
whether a model is ready for governance publication and operational integration.
}
\normative{
Evaluation protocols SHALL cover quantitative and qualitative metrics aligned with charter commitments. Results MUST be documented
in (AEIP §O.5) including methodology, datasets, statistical significance, and reviewer notes. Models SHALL NOT advance if they fail
to meet fairness or safety thresholds. Validation teams SHALL include community observers for sensitive applications. Exceptions
MAY be granted under Appendix B emergency procedures but SHALL undergo retrospective review.
}

\section*{Documentation and Explainability}
\narrative{
Explainability makes models intelligible to stakeholders. Layer 3 requires model cards, interpretability analyses, and narrative
explanations accessible to civic audiences. Persona Architecture v2 helps tailor explanations to community contexts. The narrative
highlights how transparent documentation builds trust and facilitates oversight.
}
\normative{
Each model SHALL ship with documentation packages, including model cards, decision traces, and limitations. Explanations MUST be
available in technical and civic formats and SHALL reference charter clauses. Documentation SHALL be stored in public registers
unless restricted by privacy constraints; in such cases, summaries SHALL be provided. Implementers SHALL update documentation after
significant changes and MUST log revisions in (AEIP §O.6).
}

\section*{Release Management}
\narrative{
Before models enter operations, release management ensures readiness. The narrative describes readiness reviews involving civic
mandates, ethical charters, and data stewardship verifications. Governance Publication teams prepare announcements, and AEIP
registries capture version hashes and attestation signatures.
}
\normative{
Release decisions SHALL require sign-off from custodians, oversight councils, and community reviewers. Approval packages MUST
include design specifications, training logs, evaluation reports, and risk mitigations. Release hashes SHALL be published in
\texttt{INTEGRITY\_NOTICE.md} and recorded in (AEIP §O.5). Deployments SHALL NOT proceed without completed release documentation and
public notification through Layer 7 channels.
}

\clearpage
